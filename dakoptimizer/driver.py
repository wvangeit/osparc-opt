#!/usr/bin/env python3

"""
Driver that wraps and executes an arbitrary function from dakota input
(parameter) file format and returns the result as a dakota (results) output
file.

The fom of the call must be:
    driver module.func inputfile [outputfile]

where:

    driver is a bash executable generated by the drivergen.py script,

    module.func is the python path to the simulation function,

    inputfile is a dakota input parameter file,

    outputfile is the dakota output results file generated by the simulation
    function.
"""

import os
import sys
import logging
from pathlib import Path
import argparse
import importlib
import json
import pandas
import dakota.interfacing

logging.basicConfig(level=logging.INFO)

sys.path.append(str(Path(__file__).resolve().parent.parent))
import tools.maps


def start():
    """start"""

    arg_parser = argparse.ArgumentParser(description="Dakota evaluator driver")

    arg_parser.add_argument(
        "settings_path", type=Path, help="Settings filename"
    )
    arg_parser.add_argument(
        "params_path", type=Path, help="Parameter filename"
    )
    arg_parser.add_argument("objs_path", type=Path, help="Objectives filename")

    args = arg_parser.parse_args()
    logging.debug(f"Evaluator driver got args: {args}")

    settings = json.loads(args.settings_path.read_text())
    py_module = importlib.import_module(settings["function"]["py_module"])
    py_function = settings["function"]["py_function"]

    py_eval = getattr(py_module, py_function)

    params = []
    for param_settings in settings["parameters"]:
        params.extend(
            {"name": f"{param_settings['prefix']}{i}"}
            for i in range(param_settings["number"])
        )
    objs = []
    for obj_settings in settings["objectives"]:
        objs.extend(
            {"name": f"{obj_settings['prefix']}{i}"}
            for i in range(obj_settings["number"])
        )

    param_names = [param["name"] for param in params]
    obj_names = [obj["name"] for obj in objs]

    map_settings = settings["map_function"]
    map_object = tools.maps.oSparcFileMap(
        Path(os.environ["DY_SIDECAR_PATH_OUTPUTS"])
        / "output_1"
        / Path(map_settings["input_file"]),
        Path(os.environ["DY_SIDECAR_PATH_INPUTS"])
        / "input_2"
        / Path(map_settings["output_file"]),
    )

    run_sim(
        py_eval,
        args.params_path,
        args.objs_path,
        param_names,
        obj_names,
        map_function=map_object.map_function,
    )


def read_dakota_params(infile, param_names):
    file_content = pandas.read_csv(
        infile, sep=None, skipinitialspace=True, index_col=1, header=None
    ).transpose()
    logging.debug(f"Parsed infile: \n{file_content}")

    return [file_content[param_name][0] for param_name in param_names]


def write_dakota_results(outdata, outfile, objs_names):
    dataframe = pandas.DataFrame(outdata, objs_names)
    dataframe["obj_name"] = dataframe.index
    dataframe.to_csv(outfile, sep=" ", index=False, header=False)


def run_sim(
    py_eval,
    infile,
    outfile,
    param_names,
    obj_names,
    map_function=lambda f, x: map(f, x),
):
    logging.debug(f"Reading from infile: \n{Path(infile).read_text()}")
    batch_params, batch_results = dakota.interfacing.read_parameters_file(
        infile, outfile, batch=True
    )
    xs = [list(params.values()) for params in batch_params]

    logging.debug(f"Running {py_eval} on: {xs}")

    if map_function:
        ys = list(map_function(py_eval, xs))
    else:
        ys = list(map(py_eval, xs))

    logging.debug(f"Evalation return objective value(s): {ys}")

    for result, y in zip(batch_results, ys):
        for response, obj in zip(result.responses(), y):
            print(response)
            response.function = obj

    batch_results.write()
    logging.debug(f"Wrote to outfile: \n{Path(outfile).read_text()}")


if __name__ == "__main__":
    start()
